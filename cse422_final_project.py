# -*- coding: utf-8 -*-
"""CSE422_final_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ODidahGafdPhwuF2PEv483UmtG0Ffd6q
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from scipy.stats import shapiro
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import RobustScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras import Input
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report, accuracy_score

"""# **Loading The Dataset**"""

consumer = pd.read_csv('/content/consumer_classification_dataset.csv')

consumer.head(5)

consumer.shape

consumer = consumer.dropna(axis = 0, subset = ['Gender'])

"""# **Encoding**"""

consumer.info()

"""**Level Encioding**"""

for col in ['Gender', 'Marital_Status', 'Device_Used']:
    consumer[col] = LabelEncoder().fit_transform(consumer[col])

"""**One Hot Encoding**"""

consumer = pd.get_dummies(consumer, columns=['Gender', 'Marital_Status', 'Device_Used'])
consumer.head()

consumer.head(5)

""" **Heat Map**"""

corr = consumer.corr()
plt.figure(figsize=(10, 6))
sns.heatmap(corr, annot=True, cmap='YlGnBu')
plt.title("Correlation Heatmap")
plt.show()

"""*Imbalanced Dataset Check*"""

sns.countplot(x='Churn', data=consumer)
plt.title("Churn Distribution")
plt.show()

"""# **Imputation**

**Checking the Null Values**
"""

consumer.isnull().sum()

"""**Distribution Test**"""

for col in ['Age', 'Income', 'Credit_Score']:
    stat, p = shapiro(consumer[col].dropna())
    print(f"{col}: p-value = {p:.4f}")

for col in ['Age', 'Income', 'Credit_Score']:
    plt.figure(figsize=(8, 4))
    sns.histplot(consumer[col], kde=True)
    plt.title(f"Distribution of {col}")
    plt.show()

"""**Missing Value Imputation**"""

impute = SimpleImputer(missing_values=np.nan, strategy='median')
impute.fit(consumer[['Age']])
consumer['Age'] = impute.transform(consumer[['Age']])

impute = SimpleImputer(missing_values=np.nan, strategy='median')
impute.fit(consumer[['Income']])
consumer['Income'] = impute.transform(consumer[['Income']])

impute = SimpleImputer(missing_values=np.nan, strategy='median')
impute.fit(consumer[['Credit_Score']])
consumer['Credit_Score'] = impute.transform(consumer[['Credit_Score']])

"""# **Scaling**

**Standard_Scaler**
"""

scaler = StandardScaler()
consumer[['Age', 'Income', 'Credit_Score']] = scaler.fit_transform(consumer[['Age', 'Income', 'Credit_Score']])

"""**MinMax_Scaler**"""

scaler = MinMaxScaler()
consumer[['Age', 'Income', 'Credit_Score']] = scaler.fit_transform(consumer[['Age', 'Income', 'Credit_Score']])

"""**Robust Scaler**"""

num_cols = ['Age', 'Income', 'Credit_Score']
scaler = RobustScaler()
consumer[num_cols] = scaler.fit_transform(consumer[num_cols])
print(consumer[num_cols].head())

"""# **Data Set Spliting**"""

X = consumer.drop('Churn', axis=1)
y = consumer['Churn']
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42,
    stratify=y
)

print(f"Training set size: {len(X_train)} samples")
print(f"Test set size: {len(X_test)} samples")
print("\nChurn distribution in Training set:")
print(y_train.value_counts(normalize=True))
print("\nChurn distribution in Test set:")
print(y_test.value_counts(normalize=True))

"""**Standard_Scaler**"""

scaler = StandardScaler()
X_scaled_d = scaler.fit_transform(X_train)
X_train_scaled = scaler.transform(X_train)

"""**MinMax_Scaler**"""

scaler = MinMaxScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)

"""**Robust Scaler**"""

scaler = RobustScaler()
X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)
X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)

"""**KNN**"""

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)
test_score = knn.score(X_test_scaled, y_test)
print(f"Test set accuracy: {test_score:.2f}")

"""**Nural Network**"""

model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train_scaled, y_train, epochs=8, batch_size=32)

logreg = LogisticRegression()
logreg.fit(X_train_scaled, y_train)
y_pred_lr = logreg.predict(X_test_scaled)

print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_lr))

results = {}

# KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train)
test_score_knn = knn.score(X_test_scaled, y_test)
results['knn'] = test_score_knn
# print(f"KNN Test set accuracy: {test_score_knn:.2f}")

# Neural Network
nn = Sequential([
    Input(shape=(X_train.shape[1],)),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')
])
nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
nn.fit(X_train_scaled, y_train, epochs=8, batch_size=32, verbose=0)
_, test_score_nn = nn.evaluate(X_test_scaled, y_test, verbose=0)
results['nn'] = test_score_nn
# print(f"Neural Network Test set accuracy: {test_score_nn:.2f}")


# Logistic Regression
logreg = LogisticRegression()
logreg.fit(X_train_scaled, y_train)
y_pred_lr = logreg.predict(X_test_scaled)
test_score_lr = accuracy_score(y_test, y_pred_lr)
results['logistic_regression'] = test_score_lr
# print("Logistic Regression Accuracy:", test_score_lr)
results

def evaluate_model(model, X_test, y_test, model_name):
    y_pred = model.predict(X_test)

    # Convert predictions to binary for Neural Network
    if model_name == "Neural Network":
        y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to 0 or 1

    # Get probability predictions
    if hasattr(model, "predict_proba"):
        y_prob = model.predict_proba(X_test)[:, 1]
    else:
        y_prob = model.predict(X_test)
        if len(y_prob.shape) > 1:
            y_prob = y_prob.ravel()

    print(f"\n--- {model_name} ---")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print(classification_report(y_test, y_pred))

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='OrRd')
    plt.title(f'{model_name} - Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

    # ROC Curve
    if y_prob is not None:
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        auc = roc_auc_score(y_test, y_prob)
        plt.figure()
        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.2f})')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ROC Curve - {model_name}')
        plt.legend(loc='lower right')
        plt.grid(True)
        plt.show()

evaluate_model(logreg, X_test_scaled, y_test, "Logistic Regression")
evaluate_model(knn, X_test_scaled, y_test, "K-Nearest Neighbors")
evaluate_model(nn, X_test_scaled, y_test, "Neural Network")